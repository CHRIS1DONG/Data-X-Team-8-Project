{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e40b740",
   "metadata": {},
   "source": [
    "### Basic Grammar Checker from GingerIt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b817900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gingerit.gingerit import GingerIt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf07e698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I is glad to has good frends like you',\n",
       " 'result': 'I am glad to have good friends like you',\n",
       " 'corrections': [{'start': 22,\n",
       "   'text': 'frends',\n",
       "   'correct': 'friends',\n",
       "   'definition': 'a person you know well and regard with affection and trust'},\n",
       "  {'start': 13, 'text': 'has', 'correct': 'have', 'definition': None},\n",
       "  {'start': 2, 'text': 'is', 'correct': 'am', 'definition': None}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'The smelt of fliwers bring back memories.'\n",
    "\n",
    "parser = GingerIt()\n",
    "text_ = parser.parse(text)\n",
    "text_['result']\n",
    "parser.parse('I is glad to has good frends like you')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3185cc6e",
   "metadata": {},
   "source": [
    "### Formality Classifier (Work in Progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e3d6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Steve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "2021-12-02 18:16:19.931090: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2021-12-02 18:16:19.931115: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.1.0/en_core_web_md-3.1.0-py3-none-any.whl (45.4 MB)\n",
      "Requirement already satisfied: spacy<3.2.0,>=3.1.0 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from en-core-web-md==3.1.0) (3.1.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (21.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (8.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.0.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.25.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (4.61.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (0.3.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.11.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (1.21.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (5.1.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2021.5.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (0.4.4)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "#Imports stuff and installs stuff takes a few secs to run\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import csv\n",
    "import numpy as np\n",
    "import spacy\n",
    "import sklearn\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "string_punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "!python -m spacy download en_core_web_md\n",
    "\n",
    "# Spacy model imported\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35087bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Active/passive voice\n",
    "\n",
    "# function to check the type of sentence\n",
    "# 0 refers to passive, 1 refers to active\n",
    "def checkForSentType(inputSentence):   \n",
    "    # running the model on sentence\n",
    "    getDocFile = nlp(inputSentence)\n",
    "    \n",
    "    # getting the syntactic dependency \n",
    "    getAllTags = [token.dep_ for token in getDocFile]\n",
    "    \n",
    "    # checking for 'agent' tag\n",
    "    checkPassiveTest = any(['agent' in sublist for sublist in getAllTags])\n",
    "    \n",
    "    # checking for 'nsubjpass' tag\n",
    "    checkPassiveTestTwo = any(['nsubjpass' in sublist for sublist in getAllTags])\n",
    "    if checkPassiveTest or checkPassiveTestTwo:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "#Importing the list of formal/informal words\n",
    "formal_words = pd.read_excel('formal_words.xlsx',header=None)\n",
    "formal_list = formal_words[0].values.tolist()\n",
    "informal_words = pd.read_excel('informal_words.xlsx',header=None)\n",
    "informal_list = formal_words[0].values.tolist()\n",
    "\n",
    "\n",
    "#Formal Pronouns\n",
    "#Often third person\n",
    "Formal_Pronouns = [\n",
    "    \"one\",\"oneself\",\"one's\", \n",
    "    \"who\",\"whom\",\"whomst\",\"whose\",\n",
    "    \"they\",\"them\",\"their\",\"theirs\",\"themself\",\"themselves\",\"theirself\",\"theirselves\",\n",
    "    \"it\",\"its\",\"itself\",\n",
    "    \"he\",\"him\",\"himself\",\"his\",\n",
    "    \"she\",\"her\",\"herself\",\"hers\"\n",
    "]\n",
    "\n",
    "#Informal Pronouns\n",
    "#Often first person\n",
    "Informal_Pronouns = [\n",
    "    \"I\",\"me\",\"mine\",\"mines\",\"my\",\"myself\",\n",
    "    \"we\",\"us\",\"ourself\",\"ourselves\",\"our\",\"ours\",\n",
    "    \"you\",\"your\",\"yourself\",\"yours\",\"yourselves\",\"y'all\",\"yall\",\"y'all's\"\n",
    "]\n",
    "\n",
    "#Feature Extracter\n",
    "def anvil_cleaner(data):\n",
    "            \n",
    "    #wored count\n",
    "    data['Word Count']= data[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    #Counting number of formal pronouns\n",
    "    data['Formal Pronoun'] = data[\"text\"].apply(\n",
    "        lambda x:len([w for w in str(x).lower().split() if w in Formal_Pronouns]))/data['Word Count']\n",
    "        \n",
    "    #Counting number of informal pronouns\n",
    "    data['Informal Pronoun'] = data[\"text\"].apply(\n",
    "        lambda x:len([w for w in str(x).lower().split() if w in Informal_Pronouns]))/data['Word Count']\n",
    "    \n",
    "    #Counting number of contractions\n",
    "    data['Contractions'] = data[\"text\"].apply(lambda x: x.count(\"'\"))/data['Word Count']\n",
    "    \n",
    "    #Identifying sentences with active voice\n",
    "    data['Active Voice'] = data['text'].apply(checkForSentType)\n",
    "    \n",
    "    #Identifying sentences with passive voice (just opposite of active, since all sentences are active or passive)\n",
    "    data['Passive Voice'] = 1 - data['text'].apply(checkForSentType)\n",
    "        \n",
    "    # Removing apostrophes so contractions are considered a single token\n",
    "    data['clean_text'] = data['text'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "    \n",
    "    # Remove punctuation and stop words and lowercase the text\n",
    "    data['clean_text'] = data['clean_text'].apply(lambda x: ' '.join(re.sub(\n",
    "        r'[^a-zA-Z]',' ',w).lower() for w in x.split() if re.sub(r'[^a-zA-Z]',' ',w).lower() not in stop_words))\n",
    "\n",
    "    #lemmatize\n",
    "    #data['clean_text'] = data['clean_text'].apply(lambda x: ' '.join(lemmatizer.lemmatize(w) for w in x.split()))\n",
    "\n",
    "\n",
    "    #now engineer the features the model expects\n",
    "    \n",
    "    #Formal words count (normalized to sentence length)\n",
    "    #data['Formal Words'] = data[\"text\"].apply(\n",
    "    #    lambda x:len([w for w in str(x).split() if w in formal_list]))/data['Word Count']\n",
    "        \n",
    "    #Informal words count (normalized to sentence length)\n",
    "    #data['Informal Words'] = data[\"text\"].apply(\n",
    "    #    lambda x:len([w for w in str(x).split() if w in informal_list]))/data['Word Count']\n",
    "        \n",
    "    #wored count\n",
    "    data['Word Count']= data[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "\n",
    "    data['Character count'] = data[\"text\"].apply(lambda x: len(str(x))) \n",
    "\n",
    "    data[\"average characters per word\"] =  data['Character count']/data['Word Count']\n",
    "\n",
    "    data['stopword count'] = data[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))/data['Word Count']\n",
    "\n",
    "\n",
    "    #feature for all the nowns in a text \n",
    "    from nltk import word_tokenize\n",
    "    all_text_without_sw = ''\n",
    "    for i in data.itertuples():\n",
    "        all_text_without_sw = all_text_without_sw +  str(i.text)\n",
    "\n",
    "    tokenized_all_text = word_tokenize(all_text_without_sw) #tokenize the text\n",
    "    list_of_tagged_words = nltk.pos_tag(tokenized_all_text) #adding POS Tags to tokenized words\n",
    "\n",
    "    set_pos  = (set(list_of_tagged_words)) # set of POS tags & words\n",
    "\n",
    "    nouns = ['NN','NNS','NNP','NNPS'] #POS tags of nouns\n",
    "    list_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  nouns, set_pos)))\n",
    "    #data['noun count'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )/data['Word Count']\n",
    "\n",
    "    # prnown count\n",
    "\n",
    "    pronouns = ['PRP','PRP$','WP','WP$'] # POS tags of pronouns\n",
    "    list_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  pronouns, set_pos)))\n",
    "    #data['pronoun count'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )/data['Word Count']\n",
    "\n",
    "    # count fo verbs\n",
    "\n",
    "    verbs = ['VB','VBD','VBG','VBN','VBP','VBZ'] #POS tags of verbs\n",
    "    list_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  verbs, set_pos)))\n",
    "    #data['verb count'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )/data['Word Count']\n",
    "\n",
    "\n",
    "    #adverb count\n",
    "\n",
    "    adverbs = ['RB','RBR','RBS','WRB'] #POS tags of adverbs\n",
    "    list_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  adverbs, set_pos)))\n",
    "    data['adverb count'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )/data['Word Count']\n",
    "\n",
    "    #Adjective count\n",
    "\n",
    "    adjectives = ['JJ','JJR','JJS'] #POS tags of adjectives\n",
    "    list_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  adjectives, set_pos)))\n",
    "    #data['adjective count'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )/data['Word Count']\n",
    "\n",
    "\n",
    "    data['punctuation count'] = data['clean_text'].apply(lambda x: len([w for w in str(x) if w in string_punctuation]))/data['Word Count']\n",
    "\n",
    "\n",
    "    data['mean sentance length'] = data['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97921826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input anvil sentence text in sentences list\n",
    "sentences = [\"How's it going?\",\n",
    "            \"It is our honest opinion that our organization is not properly positioned to invest in that platform.\",\n",
    "            \"We have reviewed your application and, unfortunately, we have decided to move forward with another applicant for this position.\",\n",
    "            \"Hello I am very nice to meet you!\",\n",
    "            \"I need help with my project.\"]\n",
    "\n",
    "#Converts that to a dataframe\n",
    "data = pd.DataFrame(sentences,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5102caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99225633e-01, 1.00000000e+00, 1.00000000e+00, 7.20758102e-01,\n",
       "       1.41217275e-04])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in trained pickle model (Logistic regression)\n",
    "with open('formality_model.pkl', 'rb') as f:\n",
    "    lr_loaded = pickle.load(f)\n",
    "clean_data = anvil_cleaner(data)\n",
    "\n",
    "#Transforms data to be inserted into model (drops text and scales)\n",
    "clean_data = clean_data.drop(['text', 'clean_text'], axis=1)\n",
    "cols = clean_data.columns\n",
    "scaler = MinMaxScaler()\n",
    "clean_data = scaler.fit_transform(clean_data)\n",
    "clean_data = pd.DataFrame(clean_data, columns=[cols])\n",
    "\n",
    "#Get probabilities of formalities (0 is informal, 1 is formal)\n",
    "lr_loaded.predict_proba(clean_data)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d39d9",
   "metadata": {},
   "source": [
    "## RNN for Informal to Formal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9833aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2b49be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from torchtext.data.metrics import bleu_score\n",
    "import sys\n",
    "\n",
    "\n",
    "def translate_sentence(model, sentence, informal, formal, device, max_length=50):\n",
    "    # print(sentence)\n",
    "\n",
    "    # sys.exit()\n",
    "\n",
    "    # Load german tokenizer\n",
    "    spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "    if type(sentence) == str:\n",
    "        tokens = [token.text.lower() for token in spacy_en(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # print(tokens)\n",
    "\n",
    "    # sys.exit()\n",
    "    # Add <SOS> and <EOS> in beginning and end respectively\n",
    "    tokens.insert(0, informal.init_token)\n",
    "    tokens.append(informal.eos_token)\n",
    "\n",
    "    # Go through each german token and convert to an index\n",
    "    text_to_indices = [informal.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    # Build encoder hidden, cell state\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(sentence_tensor)\n",
    "\n",
    "    outputs = [formal.vocab.stoi[\"<sos>\"]]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
    "            best_guess = output.argmax(1).item()\n",
    "\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        # Model predicts it's the end of the sentence\n",
    "        if output.argmax(1).item() == formal.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    translated_sentence = [formal.vocab.itos[idx] for idx in outputs]\n",
    "\n",
    "    # remove start token\n",
    "    return translated_sentence[1:]\n",
    "\n",
    "\n",
    "def bleu(data, model, informal, formal, device):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "\n",
    "    for example in data:\n",
    "        src = vars(example)[\"src\"]\n",
    "        trg = vars(example)[\"trg\"]\n",
    "\n",
    "        prediction = translate_sentence(model, src, informal, formal, device)\n",
    "        prediction = prediction[:-1]  # remove <eos> token\n",
    "\n",
    "        targets.append([trg])\n",
    "        outputs.append(prediction)\n",
    "\n",
    "    return bleu_score(outputs, targets)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eab9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abb22884",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenizer_en(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "informal = Field(tokenize=tokenizer_en, \n",
    "                 lower=True, \n",
    "                 init_token='<sos>', \n",
    "                 eos_token='<eos>')\n",
    "formal = Field(tokenize=tokenizer_en, \n",
    "               lower=True, \n",
    "               init_token='<sos>', \n",
    "               eos_token='<eos>')\n",
    "\n",
    "new_fields = {'informal': (\"src\", informal), 'formal': (\"trg\", formal)}\n",
    "\n",
    "train_data, validation_data, test_data = TabularDataset.splits(path='data', \n",
    "                                              train='train.csv', validation='valid.csv', \n",
    "                                              test='test.csv', \n",
    "                                              format='csv', \n",
    "                                              fields=new_fields)\n",
    "\n",
    "informal.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "formal.build_vocab(train_data, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17f8ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, drop_p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=drop_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N)\n",
    "#         print(\"x encode\", x.shape)\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        \n",
    "#         print(\"embedding encode\", embedding.shape)\n",
    "#         print(\"hidden encode\", hidden.shape)\n",
    "#         print(\"cell encode\", cell.shape)\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e1a7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, drop_p):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        #output_size=input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, \n",
    "                           hidden_size, \n",
    "                           num_layers, \n",
    "                           dropout=drop_p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape : N, want (1, N)\n",
    "        # translating one word at a time\n",
    "#         print(\"x before decode\", x.shape)\n",
    "        x = x.unsqueeze(0)\n",
    "#         print(\"x after decode\", x.shape)\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "#         print('hidden:', hidden.shape)\n",
    "#         print('cell:', cell.shape)\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        \n",
    "        predictions = self.fc(outputs)\n",
    "        \n",
    "        predictions = predictions.squeeze(0)\n",
    "        \n",
    "        return predictions, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53b8f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        # teacher determines chance to use predicted translation rather than target translation\n",
    "        # prevents overtraining on data, test time it will see very diff words from train\n",
    "        \n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(formal.vocab)\n",
    "        \n",
    "        outputs = torch.zeros(target_len, \n",
    "                              batch_size, \n",
    "                              target_vocab_size).to(device)\n",
    "        \n",
    "        # run things into encoder to get hidden and cell, then run those through decoder\n",
    "        hidden, cell = self.encoder(source)\n",
    "        \n",
    "        # start token\n",
    "        x = target[0]\n",
    "        \n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            \n",
    "            outputs[t] = output\n",
    "            \n",
    "            best_guess = output.argmax(1)\n",
    "            \n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7fc0dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "# training hyperparam\n",
    "\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "# model hyperparam\n",
    "\n",
    "load_model = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size_encoder = len(informal.vocab)\n",
    "input_size_decoder = len(formal.vocab)\n",
    "output_size = len(formal.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "# tensorboard\n",
    "writer = SummaryWriter(f'runs/loss_plot')\n",
    "step = 0\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, \n",
    "                                                                      validation_data, \n",
    "                                                                      test_data), \n",
    "                                                                      batch_size=batch_size, \n",
    "                                                                      sort_within_batch=True, \n",
    "                                                                      sort_key= lambda x: len(x.src), \n",
    "                                                                      device=device)\n",
    "\n",
    "encoder_net = Encoder(input_size_encoder, \n",
    "                      encoder_embedding_size, \n",
    "                      hidden_size, \n",
    "                      num_layers, \n",
    "                      enc_dropout).to(device)\n",
    "\n",
    "decoder_net = Decoder(input_size_decoder, \n",
    "                      decoder_embedding_size, \n",
    "                      hidden_size, \n",
    "                      output_size, \n",
    "                      num_layers, \n",
    "                      dec_dropout).to(device)\n",
    "\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "pad_idx = formal.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "if load_model:\n",
    "    load_checkpoint(torch.load('my_checkpoint.pth.tar'), model, optimizer)\n",
    "    \n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f'Epoch [{epoch}/{num_epochs}]')\n",
    "    \n",
    "#     checkpoint = {'state_dict':model.state_dict(), 'optimizer':optimizer.state_dict()}\n",
    "#     save_checkpoint(checkpoint)\n",
    "    \n",
    "#     for batch_idx, batch in enumerate(train_iterator):\n",
    "#         inp_data = batch.src.to(device)\n",
    "#         target = batch.trg.to(device)\n",
    "        \n",
    "#         output = model(inp_data, target)\n",
    "#         # output: (trg_len, batch_size, output_dim)\n",
    "        \n",
    "#         output = output[1:].reshape(-1, output.shape[2])\n",
    "#         target = target[1:].reshape(-1)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss = criterion(output, target)\n",
    "        \n",
    "#         loss.backward()\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1)\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         writer.add_scalar('Training Loss', loss, global_step=step)\n",
    "#         step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e534c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_formal(sentence):\n",
    "    lst = translate_sentence(model, sentence, informal, formal, device)\n",
    "    final = \"\"\n",
    "    for i in range(len(lst)-1):\n",
    "        if i == len(lst)-2:\n",
    "            final = final.strip()\n",
    "        final = final + lst[i] + \" \"\n",
    "    final = final.strip()\n",
    "    return final.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "812bea1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How is anyway?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_formal(\"How's life?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca174d0f",
   "metadata": {},
   "source": [
    "## Sentence Level Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "233f08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "model_translate = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer_translate = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a787cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(tokens_tensor):\n",
    "    loss=model_translate(tokens_tensor, labels=tokens_tensor)[0]\n",
    "    return np.exp(loss.cpu().detach().numpy())\n",
    "\n",
    "def GPT2_Sentence_Scorer(text):\n",
    "    scores = []\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        scores.append((score(tokenizer_translate.encode(sent, add_special_tokens=False, return_tensors=\"pt\")), sent))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5cb6b4",
   "metadata": {},
   "source": [
    "## Translation Models From HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9810991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from transformers import DistilBertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1e4ccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN=\"api_oMMRDkimUuBHSmsDNophzfNCmbzfimIFRl\"\n",
    "\n",
    "models_dictionary = {'English to Spanish':\"https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-en-es\",\n",
    "                    'Spanish to English':\"https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-es-en\",\n",
    "                    'English to French':\"https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-en-fr\",\n",
    "                    'French to English':\"https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-fr-en\",\n",
    "                    'English to Chinese':\"https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-en-zh\",\n",
    "                    'Chinese to English':\"https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-zh-en\"}\n",
    "\n",
    "def query(payload, API_URL, headers):\n",
    "    data = json.dumps(payload)\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    return json.loads(response.content.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f73dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disconnecting from previous connection first...\n",
      "Connecting to wss://anvil.works/uplink\n",
      "Anvil websocket closed (code 1000, reason=b'')\n",
      "Anvil websocket open\n",
      "Connected to \"Default environment\" as SERVER\n",
      "{'error': 'Model Helsinki-NLP/opus-mt-en-es is currently loading', 'estimated_time': 20}\n",
      "[{'translation_text': 'Hola.'}]\n",
      "{'error': 'Model Helsinki-NLP/opus-mt-en-fr is currently loading', 'estimated_time': 20}\n",
      "[{'translation_text': 'Bonjour.'}]\n",
      "{'error': 'Model Helsinki-NLP/opus-mt-en-zh is currently loading', 'estimated_time': 20}\n",
      "[{'translation_text': '喂 喂 喂 喂 喂 喂 喂 喂 喂 喂 喂 喂 喂 喂 喂 喂 喂 喂 喂 喂 喂 喂 喂'}]\n",
      "[(166.31491, 'Subsequently the team parsed, cleaned and normalized data and to optimize it for the advanced deep learning powered language models they use to generate their suggestions.'), (39.54744, 'The final product is displayed on an interactive, easy to use platform where users can write their messages from scratch, receiving feedback as they go.'), (7806.339, 'This not most sounding natural text example-using project..'), (47.42845, 'I need help with my project.')]\n",
      "[(166.31491, 'Subsequently the team parsed, cleaned and normalized data and to optimize it for the advanced deep learning powered language models they use to generate their suggestions.'), (39.54744, 'The final product is displayed on an interactive, easy to use platform where users can write their messages from scratch, receiving feedback as they go.'), (7806.339, 'This not most sounding natural text example-using project..'), (47.42845, 'I need help with my project.')]\n",
      "[(166.31491, 'Subsequently the team parsed, cleaned and normalized data and to optimize it for the advanced deep learning powered language models they use to generate their suggestions.'), (39.54744, 'The final product is displayed on an interactive, easy to use platform where users can write their messages from scratch, receiving feedback as they go.'), (7806.339, 'This not most sounding natural text example-using project..'), (47.42845, 'I need help with my project.')]\n",
      "[(166.31491, 'Subsequently the team parsed, cleaned and normalized data and to optimize it for the advanced deep learning powered language models they use to generate their suggestions.'), (39.54744, 'The final product is displayed on an interactive, easy to use platform where users can write their messages from scratch, receiving feedback as they go.'), (7806.339, 'This not most sounding natural text example-using project..'), (47.42845, 'I need help with my project.')]\n",
      "[(166.31491, 'Subsequently the team parsed, cleaned and normalized data and to optimize it for the advanced deep learning powered language models they use to generate their suggestions.'), (112.578896, 'The final product is displayed on an interactive, easy to use \\nplatform where users can write their messages from scratch, receiving feedback as they go.'), (7806.339, 'This not most sounding natural text example-using project..'), (47.42845, 'I need help with my project.')]\n",
      "[(166.31491, 'Subsequently the team parsed, cleaned and normalized data and to optimize it for the advanced deep learning powered language models they use to generate their suggestions.'), (112.578896, 'The final product is displayed on an interactive, easy to use \\nplatform where users can write their messages from scratch, receiving feedback as they go.'), (7806.339, 'This not most sounding natural text example-using project..'), (47.42845, 'I need help with my project.')]\n",
      "[(166.31491, 'Subsequently the team parsed, cleaned and normalized data and to optimize it for the advanced deep learning powered language models they use to generate their suggestions.'), (112.578896, 'The final product is displayed on an interactive, easy to use \\nplatform where users can write their messages from scratch, receiving feedback as they go.'), (7806.339, 'This not most sounding natural text example-using project..'), (47.42845, 'I need help with my project.')]\n",
      "[(166.31491, 'Subsequently the team parsed, cleaned and normalized data and to optimize it for the advanced deep learning powered language models they use to generate their suggestions.'), (112.578896, 'The final product is displayed on an interactive, easy to use \\nplatform where users can write their messages from scratch, receiving feedback as they go.'), (7806.339, 'This not most sounding natural text example-using project..'), (47.42845, 'I need help with my project.')]\n",
      "[(166.31491, 'Subsequently the team parsed, cleaned and normalized data and to optimize it for the advanced deep learning powered language models they use to generate their suggestions.'), (112.578896, 'The final product is displayed on an interactive, easy to use \\nplatform where users can write their messages from scratch, receiving feedback as they go.'), (7806.339, 'This not most sounding natural text example-using project..'), (47.42845, 'I need help with my project.')]\n",
      "[(166.31491, 'Subsequently the team parsed, cleaned and normalized data and to optimize it for the advanced deep learning powered language models they use to generate their suggestions.'), (112.578896, 'The final product is displayed on an interactive, easy to use \\nplatform where users can write their messages from scratch, receiving feedback as they go.'), (7806.339, 'This not most sounding natural text example-using project..'), (47.42845, 'I need help with my project.')]\n",
      "[(166.31491, 'Subsequently the team parsed, cleaned and normalized data and to optimize it for the advanced deep learning powered language models they use to generate their suggestions.'), (39.54744, 'The final product is displayed on an interactive, easy to use platform where users can write their messages from scratch, receiving feedback as they go.'), (7806.339, 'This not most sounding natural text example-using project..'), (47.42845, 'I need help with my project.')]\n",
      "[(166.31491, 'Subsequently the team parsed, cleaned and normalized data and to optimize it for the advanced deep learning powered language models they use to generate their suggestions.'), (39.54744, 'The final product is displayed on an interactive, easy to use platform where users can write their messages from scratch, receiving feedback as they go.'), (7806.339, 'This not most sounding natural text example-using project..'), (47.42845, 'I need help with my project.')]\n"
     ]
    }
   ],
   "source": [
    "import anvil.server\n",
    "\n",
    "anvil.server.connect(\"WY53G5FKUA3OMQSIHJ2GCQLL-NVAGOPE44XRT7GXJ\")\n",
    "\n",
    "@anvil.server.callable\n",
    "def grammar_checker(text):\n",
    "    if parser.parse(text)['corrections']:\n",
    "        return parser.parse(text)['result']\n",
    "    else:\n",
    "        return 'No corrections needed.'\n",
    "    \n",
    "@anvil.server.callable    \n",
    "def formality_checker(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    data = pd.DataFrame(sentences,columns=['text'])\n",
    "    clean_data = anvil_cleaner(data)\n",
    "\n",
    "    #Transforms data to be inserted into model (drops text and scales)\n",
    "    clean_data = clean_data.drop(['text', 'clean_text'], axis=1)\n",
    "    cols = clean_data.columns\n",
    "    scaler = MinMaxScaler()\n",
    "    clean_data = scaler.fit_transform(clean_data)\n",
    "    clean_data = pd.DataFrame(clean_data, columns=[cols])\n",
    "\n",
    "    #Get probabilities of formalities (0 is informal, 1 is formal)\n",
    "    vals = list((zip(lr_loaded.predict_proba(clean_data)[:,1], sentences)))\n",
    "    output = ''\n",
    "    for val in vals:\n",
    "        if val[0] > 0.5:\n",
    "            output = output + \"Formal:  \" + val[1] + '\\n'\n",
    "        else:\n",
    "            output = output + \"Informal:  \" + val[1] + '\\n'\n",
    "        output = output + '\\n'\n",
    "    return output\n",
    "\n",
    "@anvil.server.callable  \n",
    "def rnn_informal_formal(x):\n",
    "    return to_formal(x)\n",
    "    \n",
    "@anvil.server.callable\n",
    "def translate(text, language):\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "    API_URL = models_dictionary[language]\n",
    "    result = query({\"inputs\":f'{text}',}, API_URL, headers)\n",
    "    while 'error' in result:\n",
    "        time.sleep(20)\n",
    "        result = query({\"inputs\":f'{text}',}, API_URL, headers)\n",
    "        print(result)\n",
    "    return result[0]['translation_text']\n",
    "\n",
    "@anvil.server.callable\n",
    "def nat_sounding(text):\n",
    "    list_of_scores = GPT2_Sentence_Scorer(text)\n",
    "    good, okay, bad = [], [], []\n",
    "    for sent in list_of_scores:\n",
    "        if sent[0] > 100 and sent[0] < 200:\n",
    "            okay.append(sent[1] + '\\n')\n",
    "        elif sent[0] >= 200:\n",
    "            bad.append(sent[1] + '\\n')\n",
    "        else:\n",
    "            good.append(sent[1] + '\\n')\n",
    "    if len(okay) == 0:\n",
    "        okay.append('None')\n",
    "    if len(bad) == 0:\n",
    "        okay.append('None')\n",
    "    print(list_of_scores)\n",
    "    return (good,okay,bad)\n",
    "    \n",
    "anvil.server.wait_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1983aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install anvil-uplink"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
