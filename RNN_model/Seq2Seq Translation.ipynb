{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfff8d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from torchtext) (1.21.3)\n",
      "Requirement already satisfied: torch==1.10.0 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from torchtext) (1.10.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from torchtext) (4.61.2)\n",
      "Requirement already satisfied: requests in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from torchtext) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from torch==1.10.0->torchtext) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->torchtext) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->torchtext) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->torchtext) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->torchtext) (4.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\steve\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tqdm->torchtext) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6bfd3b5",
   "metadata": {
    "executionInfo": {
     "elapsed": 6125,
     "status": "ok",
     "timestamp": 1638496037193,
     "user": {
      "displayName": "Steven Johannemann",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04939645169924988826"
     },
     "user_tz": 480
    },
    "id": "e6bfd3b5"
   },
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1a3cd02",
   "metadata": {
    "executionInfo": {
     "elapsed": 156,
     "status": "ok",
     "timestamp": 1638496049253,
     "user": {
      "displayName": "Steven Johannemann",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04939645169924988826"
     },
     "user_tz": 480
    },
    "id": "e1a3cd02"
   },
   "outputs": [],
   "source": [
    "# tokenize = lambda x: x.split()\n",
    "\n",
    "# informal = Field(sequential=True, use_vocab=True, tokenize=tokenize, lower=True)\n",
    "# formal = Field(sequential=True, use_vocab=True, tokenize=tokenize, lower=True)\n",
    "\n",
    "# fields = {'informal': (\"i\", informal), 'formal': (\"f\", formal)}\n",
    "\n",
    "# train_data, test_data = TabularDataset.splits(path='data', \n",
    "#                                               train='train.csv', \n",
    "#                                               test='test.csv', \n",
    "#                                               format='csv', \n",
    "#                                               fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7aa4f41",
   "metadata": {
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1638496050712,
     "user": {
      "displayName": "Steven Johannemann",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04939645169924988826"
     },
     "user_tz": 480
    },
    "id": "d7aa4f41"
   },
   "outputs": [],
   "source": [
    "# type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d502b214",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1638496051255,
     "user": {
      "displayName": "Steven Johannemann",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04939645169924988826"
     },
     "user_tz": 480
    },
    "id": "d502b214"
   },
   "outputs": [],
   "source": [
    "# informal.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "\n",
    "# train_iterator, test_iterator = BucketIterator.splits((train_data, test_data), \n",
    "#                                                       batch_size=2, \n",
    "#                                                       device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c3d2e5",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1638496052194,
     "user": {
      "displayName": "Steven Johannemann",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04939645169924988826"
     },
     "user_tz": 480
    },
    "id": "d4c3d2e5"
   },
   "outputs": [],
   "source": [
    "# source: https://www.youtube.com/watch?v=EoGUlvhRYpk&list=PLhhyoLH6Ijfyl_VMCsi54UqGQafGkNOQH&index=1\n",
    "# github: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/ac5dcd03a40a08a8af7e1a67ade37f28cf88db43/ML/Pytorch/more_advanced/Seq2Seq/utils.py#L7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc6863f7",
   "metadata": {
    "executionInfo": {
     "elapsed": 745,
     "status": "ok",
     "timestamp": 1638496054295,
     "user": {
      "displayName": "Steven Johannemann",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04939645169924988826"
     },
     "user_tz": 480
    },
    "id": "bc6863f7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from torchtext.data.metrics import bleu_score\n",
    "import sys\n",
    "\n",
    "\n",
    "def translate_sentence(model, sentence, informal, formal, device, max_length=50):\n",
    "    # print(sentence)\n",
    "\n",
    "    # sys.exit()\n",
    "\n",
    "    # Load german tokenizer\n",
    "    spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "    if type(sentence) == str:\n",
    "        tokens = [token.text.lower() for token in spacy_en(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # print(tokens)\n",
    "\n",
    "    # sys.exit()\n",
    "    # Add <SOS> and <EOS> in beginning and end respectively\n",
    "    tokens.insert(0, informal.init_token)\n",
    "    tokens.append(informal.eos_token)\n",
    "\n",
    "    # Go through each german token and convert to an index\n",
    "    text_to_indices = [informal.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    # Build encoder hidden, cell state\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(sentence_tensor)\n",
    "\n",
    "    outputs = [formal.vocab.stoi[\"<sos>\"]]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
    "            best_guess = output.argmax(1).item()\n",
    "\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        # Model predicts it's the end of the sentence\n",
    "        if output.argmax(1).item() == formal.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    translated_sentence = [formal.vocab.itos[idx] for idx in outputs]\n",
    "\n",
    "    # remove start token\n",
    "    return translated_sentence[1:]\n",
    "\n",
    "\n",
    "def bleu(data, model, informal, formal, device):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "\n",
    "    for example in data:\n",
    "        src = vars(example)[\"src\"]\n",
    "        trg = vars(example)[\"trg\"]\n",
    "\n",
    "        prediction = translate_sentence(model, src, informal, formal, device)\n",
    "        prediction = prediction[:-1]  # remove <eos> token\n",
    "\n",
    "        targets.append([trg])\n",
    "        outputs.append(prediction)\n",
    "\n",
    "    return bleu_score(outputs, targets)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f5b7b",
   "metadata": {
    "id": "a20f5b7b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9672e7b7",
   "metadata": {
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1638496058271,
     "user": {
      "displayName": "Steven Johannemann",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04939645169924988826"
     },
     "user_tz": 480
    },
    "id": "9672e7b7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7d12c94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "executionInfo": {
     "elapsed": 1263,
     "status": "error",
     "timestamp": 1638496061802,
     "user": {
      "displayName": "Steven Johannemann",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04939645169924988826"
     },
     "user_tz": 480
    },
    "id": "c7d12c94",
    "outputId": "a298848f-bdb4-4853-ec85-827cd685abac"
   },
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenizer_en(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "informal = Field(tokenize=tokenizer_en, \n",
    "                 lower=True, \n",
    "                 init_token='<sos>', \n",
    "                 eos_token='<eos>')\n",
    "formal = Field(tokenize=tokenizer_en, \n",
    "               lower=True, \n",
    "               init_token='<sos>', \n",
    "               eos_token='<eos>')\n",
    "\n",
    "new_fields = {'informal': (\"src\", informal), 'formal': (\"trg\", formal)}\n",
    "\n",
    "train_data, validation_data, test_data = TabularDataset.splits(path='data', \n",
    "                                              train='train.csv', validation='valid.csv', \n",
    "                                              test='test.csv', \n",
    "                                              format='csv', \n",
    "                                              fields=new_fields)\n",
    "\n",
    "informal.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "formal.build_vocab(train_data, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82154311",
   "metadata": {
    "id": "82154311"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, drop_p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=drop_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N)\n",
    "#         print(\"x encode\", x.shape)\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        \n",
    "#         print(\"embedding encode\", embedding.shape)\n",
    "#         print(\"hidden encode\", hidden.shape)\n",
    "#         print(\"cell encode\", cell.shape)\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35b5304b",
   "metadata": {
    "id": "35b5304b"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, drop_p):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        #output_size=input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, \n",
    "                           hidden_size, \n",
    "                           num_layers, \n",
    "                           dropout=drop_p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape : N, want (1, N)\n",
    "        # translating one word at a time\n",
    "#         print(\"x before decode\", x.shape)\n",
    "        x = x.unsqueeze(0)\n",
    "#         print(\"x after decode\", x.shape)\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "#         print('hidden:', hidden.shape)\n",
    "#         print('cell:', cell.shape)\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        \n",
    "        predictions = self.fc(outputs)\n",
    "        \n",
    "        predictions = predictions.squeeze(0)\n",
    "        \n",
    "        return predictions, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be984aa4",
   "metadata": {
    "id": "be984aa4"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        # teacher determines chance to use predicted translation rather than target translation\n",
    "        # prevents overtraining on data, test time it will see very diff words from train\n",
    "        \n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(formal.vocab)\n",
    "        \n",
    "        outputs = torch.zeros(target_len, \n",
    "                              batch_size, \n",
    "                              target_vocab_size).to(device)\n",
    "        \n",
    "        # run things into encoder to get hidden and cell, then run those through decoder\n",
    "        hidden, cell = self.encoder(source)\n",
    "        \n",
    "        # start token\n",
    "        x = target[0]\n",
    "        \n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            \n",
    "            outputs[t] = output\n",
    "            \n",
    "            best_guess = output.argmax(1)\n",
    "            \n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57b7d108",
   "metadata": {
    "id": "57b7d108",
    "outputId": "8d39d392-e4fa-454d-83f7-84a1c81be070"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "# training hyperparam\n",
    "\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "# model hyperparam\n",
    "\n",
    "load_model = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size_encoder = len(informal.vocab)\n",
    "input_size_decoder = len(formal.vocab)\n",
    "output_size = len(formal.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "# tensorboard\n",
    "writer = SummaryWriter(f'runs/loss_plot')\n",
    "step = 0\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, \n",
    "                                                                      validation_data, \n",
    "                                                                      test_data), \n",
    "                                                                      batch_size=batch_size, \n",
    "                                                                      sort_within_batch=True, \n",
    "                                                                      sort_key= lambda x: len(x.src), \n",
    "                                                                      device=device)\n",
    "\n",
    "encoder_net = Encoder(input_size_encoder, \n",
    "                      encoder_embedding_size, \n",
    "                      hidden_size, \n",
    "                      num_layers, \n",
    "                      enc_dropout).to(device)\n",
    "\n",
    "decoder_net = Decoder(input_size_decoder, \n",
    "                      decoder_embedding_size, \n",
    "                      hidden_size, \n",
    "                      output_size, \n",
    "                      num_layers, \n",
    "                      dec_dropout).to(device)\n",
    "\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "pad_idx = formal.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "if load_model:\n",
    "    load_checkpoint(torch.load('my_checkpoint.pth.tar'), model, optimizer)\n",
    "    \n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f'Epoch [{epoch}/{num_epochs}]')\n",
    "    \n",
    "#     checkpoint = {'state_dict':model.state_dict(), 'optimizer':optimizer.state_dict()}\n",
    "#     save_checkpoint(checkpoint)\n",
    "    \n",
    "#     for batch_idx, batch in enumerate(train_iterator):\n",
    "#         inp_data = batch.src.to(device)\n",
    "#         target = batch.trg.to(device)\n",
    "        \n",
    "#         output = model(inp_data, target)\n",
    "#         # output: (trg_len, batch_size, output_dim)\n",
    "        \n",
    "#         output = output[1:].reshape(-1, output.shape[2])\n",
    "#         target = target[1:].reshape(-1)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss = criterion(output, target)\n",
    "        \n",
    "#         loss.backward()\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1)\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         writer.add_scalar('Training Loss', loss, global_step=step)\n",
    "#         step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28819952",
   "metadata": {
    "id": "28819952",
    "outputId": "59f9862c-849b-4e82-e02a-daeba25388b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'going', 'to', 'be', '.', '.', '<eos>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sentence(model, \"Honey, I will be back.\", informal, formal, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2520747",
   "metadata": {
    "id": "c2520747"
   },
   "outputs": [],
   "source": [
    "def to_formal(sentence):\n",
    "    lst = translate_sentence(model, sentence, informal, formal, device)\n",
    "    final = \"\"\n",
    "    for i in range(len(lst)-1):\n",
    "        if i == len(lst)-2:\n",
    "            final = final.strip()\n",
    "        final = final + lst[i] + \" \"\n",
    "    final = final.strip()\n",
    "    return final.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46882d1d",
   "metadata": {
    "id": "46882d1d",
    "outputId": "a98e89a0-1dc0-4541-da75-c18f05ff57c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How is your life?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_formal(\"How's life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8383c035",
   "metadata": {
    "id": "8383c035"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b19bbc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Seq2Seq Translation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
